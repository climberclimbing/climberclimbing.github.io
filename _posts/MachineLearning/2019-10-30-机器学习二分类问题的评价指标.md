---
layout: post
title: 机器学习二分类问题的评价指标
categories: MachineLearning
description: 机器学习二分类问题的评价指标
keywords: 机器学习 评价指标
---

## 混淆矩阵

|   | 实际为正例  | 实际为负例 |
| :------------ |:---------------:| -----:|
| 预测为正例      | True Positive(TP) | False Positive(FP) |
| 预测为负例      | False Negative(FN)        |   True Negative(TN) |

## Accuracy(准确率)

准确率的定义是预测正确的样本占总预测样本的百分比。其公式如下：
```
$$Accuracy=\frac{TP+TN}{TP+TN+FP+FN}$$
```
## Precision(精确率)

精确率是针对我们预测结果而言的，它表示的是预测为正的样本中有多少是真正的正样本。其公式如下：
```
$$Precision=\frac{TP}{TP+FP}$$
```
而召回率是针对我们所要预测的样本而言的，它表示的是样本中的正例有多少被预测正确了。其公式如下：

## Recall(召回率)

召回率是针对我们原来的样本而言的，它表示的是样本中的正例有多少被预测正确了。
```
$$Recall=\frac{TP}{TP+FN}$$
```
## ROC与AUC

ROC与AUC的理解参考[4][5]

ROC曲线的绘制参考[6]

AUC的计算方法参考[7]

## 讨论

虽然Accuracy可以判断总的正确率，但是在样本不平衡的情况下，并不能作为很好的指标来衡量分类模型。举个简单的例子，比如在一个总样本中，正样本占90%，负样本占10%，样本是严重不平衡的。对于这种情况，我们只需要将全部样本预测为正样本即可得到90%的高准确率。所以样本不平衡的情况下，我们需要考虑使用Precision和Recall来衡量分类模型。

Precision和Recall直观理解如下图：

![准确率与召回率](https:/climberclimbing.github.io/images/PrecisionAndRecall.jfif)

实际上，对于同一个分类模型，我们不能同时提高Precision和Recall，它们之间是负相关的。我们需要在这两者之间进行一个折中考虑，由此引入F1值。其计算公式如下：
```
$$F1 Score=\frac{2*Precision*Recall}{Precision+Recall}$$
```
F1值使用了调和平均值，其目的是为了“惩罚”Precision与Recall之间的差距，即Precision与Recall之间的差距越大，F1 Score减小的越厉害。

## 参考

[1] [如何解释召回率与精确率？ - Charles Xiao的回答](https://www.zhihu.com/question/19645541/answer/91694636)

[2] [机器学习中的F1度量，为什么定义为precision和recall的调和平均，而不是算术平均？](https://www.zhihu.com/question/47980482/answer/558863976)

[3] [调和平均](http://www.di.unipi.it/~bozzo/The%20Harmonic%20Mean.htm)

[4] [ROC曲线和AUC面积理解](https://blog.csdn.net/program_developer/article/details/79946787)

[5] [全面了解ROC曲线](https://www.plob.org/article/12476.html)

[6] [如何画ROC曲线](https://blog.csdn.net/xiaohuihui1994/article/details/87987836)

[7] [AUC值的含义与计算方法](https://baijiahao.baidu.com/s?id=1597939133517926460&wfr=spider&for=pc)

